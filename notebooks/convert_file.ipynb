{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from fastavro import writer, parse_schema\n",
    "import json\n",
    "from sqlalchemy import create_engine\n",
    "from datahub.metadata.schema_classes import MetadataChangeEventClass\n",
    "from datahub.emitter.mce_builder import make_dataset_urn\n",
    "from datahub.emitter.rest_emitter import DatahubRestEmitter\n",
    "from grafana_api.grafana_face import GrafanaFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for converting CSV and JSON to Parquet and Avro formats\n",
    "\n",
    "def csv_to_parquet(csv_file, parquet_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    pq.write_table(table, parquet_file)\n",
    "    print(f\"CSV file {csv_file} successfully converted to Parquet format at {parquet_file}\")\n",
    "\n",
    "def json_to_parquet(json_file, parquet_file):\n",
    "    df = pd.read_json(json_file)\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    pq.write_table(table, parquet_file)\n",
    "    print(f\"JSON file {json_file} successfully converted to Parquet format at {parquet_file}\")\n",
    "\n",
    "def csv_to_avro(csv_file, avro_file, schema):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    records = df.to_dict(orient='records')\n",
    "    parsed_schema = parse_schema(schema)\n",
    "    with open(avro_file, 'wb') as out:\n",
    "        writer(out, parsed_schema, records)\n",
    "    print(f\"CSV file {csv_file} successfully converted to Avro format at {avro_file}\")\n",
    "\n",
    "def json_to_avro(json_file, avro_file, schema):\n",
    "    with open(json_file) as f:\n",
    "        records = json.load(f)\n",
    "    parsed_schema = parse_schema(schema)\n",
    "    with open(avro_file, 'wb') as out:\n",
    "        writer(out, parsed_schema, records)\n",
    "    print(f\"JSON file {json_file} successfully converted to Avro format at {avro_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for saving data to database\n",
    "\n",
    "def save_parquet_to_db(parquet_file, db_uri, table_name):\n",
    "    df = pd.read_parquet(parquet_file)\n",
    "    engine = create_engine(db_uri)\n",
    "    df.to_sql(table_name, engine, if_exists='replace')\n",
    "    print(f\"Parquet file {parquet_file} successfully saved to database table {table_name}\")\n",
    "\n",
    "def save_avro_to_db(avro_file, db_uri, table_name, schema):\n",
    "    with open(avro_file, 'rb') as f:\n",
    "        reader = reader(f, schema)\n",
    "        records = [record for record in reader]\n",
    "    df = pd.DataFrame(records)\n",
    "    engine = create_engine(db_uri)\n",
    "    df.to_sql(table_name, engine, if_exists='replace')\n",
    "    print(f\"Avro file {avro_file} successfully saved to database table {table_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file ../data/csv/profile.csv successfully converted to Parquet format at ../data/parquet/example_csv.parquet\n",
      "JSON file ../data/json/employee.json successfully converted to Parquet format at ../data/parquet/example_json.parquet\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "csv_file = '../data/csv/profile.csv'\n",
    "json_file = '../data/json/employee.json'\n",
    "parquet_file_csv = '../data/parquet/example_csv.parquet'\n",
    "parquet_file_json = '../data/parquet/example_json.parquet'\n",
    "avro_file_csv = 'example_csv.avro'\n",
    "avro_file_json = 'example_json.avro'\n",
    "\n",
    "# Example Avro schema\n",
    "schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"example\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"field1\", \"type\": \"string\"},\n",
    "        {\"name\": \"field2\", \"type\": \"int\"},\n",
    "        # Add more fields as per your data\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Convert CSV and JSON to Parquet and Avro formats\n",
    "csv_to_parquet(csv_file, parquet_file_csv)\n",
    "json_to_parquet(json_file, parquet_file_json)\n",
    "# csv_to_avro(csv_file, avro_file_csv, schema)\n",
    "# json_to_avro(json_file, avro_file_json, schema)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-helper-E9HeZA3E-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
