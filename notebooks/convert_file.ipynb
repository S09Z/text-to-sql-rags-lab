{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from fastavro import writer, parse_schema\n",
    "import json\n",
    "from sqlalchemy import create_engine\n",
    "from datahub.metadata.schema_classes import MetadataChangeEventClass\n",
    "from datahub.emitter.mce_builder import make_dataset_urn\n",
    "from datahub.emitter.rest_emitter import DatahubRestEmitter\n",
    "from grafana_api.grafana_face import GrafanaFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for converting CSV and JSON to Parquet and Avro formats\n",
    "\n",
    "def csv_to_parquet(csv_file, parquet_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    pq.write_table(table, parquet_file)\n",
    "    print(f\"CSV file {csv_file} successfully converted to Parquet format at {parquet_file}\")\n",
    "\n",
    "def json_to_parquet(json_file, parquet_file):\n",
    "    df = pd.read_json(json_file)\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    pq.write_table(table, parquet_file)\n",
    "    print(f\"JSON file {json_file} successfully converted to Parquet format at {parquet_file}\")\n",
    "\n",
    "def csv_to_avro(csv_file, avro_file, schema):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    records = df.to_dict(orient='records')\n",
    "    parsed_schema = parse_schema(schema)\n",
    "    with open(avro_file, 'wb') as out:\n",
    "        writer(out, parsed_schema, records)\n",
    "    print(f\"CSV file {csv_file} successfully converted to Avro format at {avro_file}\")\n",
    "\n",
    "def json_to_avro(json_file, avro_file, schema):\n",
    "    with open(json_file) as f:\n",
    "        records = json.load(f)\n",
    "    parsed_schema = parse_schema(schema)\n",
    "    with open(avro_file, 'wb') as out:\n",
    "        writer(out, parsed_schema, records)\n",
    "    print(f\"JSON file {json_file} successfully converted to Avro format at {avro_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for saving data to database\n",
    "\n",
    "def save_parquet_to_db(parquet_file, db_uri, table_name):\n",
    "    df = pd.read_parquet(parquet_file)\n",
    "    engine = create_engine(db_uri)\n",
    "    df.to_sql(table_name, engine, if_exists='replace')\n",
    "    print(f\"Parquet file {parquet_file} successfully saved to database table {table_name}\")\n",
    "\n",
    "def save_avro_to_db(avro_file, db_uri, table_name, schema):\n",
    "    with open(avro_file, 'rb') as f:\n",
    "        reader = reader(f, schema)\n",
    "        records = [record for record in reader]\n",
    "    df = pd.DataFrame(records)\n",
    "    engine = create_engine(db_uri)\n",
    "    df.to_sql(table_name, engine, if_exists='replace')\n",
    "    print(f\"Avro file {avro_file} successfully saved to database table {table_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
